{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bithomedefaulcondac3ef0b2983f64cfd8cf91c045a31b21e",
   "display_name": "Python 3.7.6 64-bit ('homedefaul': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Importance ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After noise and correlation detection was made, and thus the n. of feateures shrinked down, we ended up with 470 features, which is still a considerable amount.\n",
    "# We therefore apply here feature selection by means of feature importance permutation in the hope of reducing even more the space of predictors and, perhaps, gain some insights towards the interpetation of the model\n",
    "# Permutation Importance tries to overcome the bias problem present in most common Gini (Impurity based) importance mechanism by permuting several times each feature and assessing how much the score, related to a validation set, is affected.\n",
    "# In particular, impurity-based feature importance for trees are strongly biased and favor high cardinality features (typically numerical features)\n",
    "# We computed pimp using LGB as a classifier since it was the one performing the best among the tried algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import sklearn\n",
    "import gc\n",
    "gc.enable()\n",
    "import lightgbm as lgb\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = utilities.read_csv('../../datatmp/data/trainjoincollnoisy.csv')\n",
    "test = utilities.read_csv('../../datatmp/data/testjoincollnoisy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['TARGET'].astype(int).copy()\n",
    "testIds = test['SK_ID_CURR'].astype(int).copy()\n",
    "\n",
    "train.drop('SK_ID_CURR', axis=1, inplace=True)\n",
    "train.drop('TARGET',axis=1, inplace=True)\n",
    "test.drop('SK_ID_CURR', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the last version of lgb, we got an error related to special chars in column names. Replacing the blanks in the following fixed it\n",
    "train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in train.columns]\n",
    "test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Train LGB and hence compute its permutation importance\n",
    "def trainNpredLgb(train, y, nFolds, pimp=True):\n",
    "\n",
    "    valPreds = np.zeros(train.shape[0])\n",
    "    importanceDf = pd.DataFrame(columns=train.columns)\n",
    "    importanceDf.loc[0] = np.zeros(train.shape[1])\n",
    "    \n",
    "    # Unbalanced dataset better to fold (use stratified K-Folds?)\n",
    "    folds = KFold(n_splits=nFolds, shuffle=True, random_state=42)\n",
    "\n",
    "    for fold, (trainIds, valIds) in enumerate(folds.split(train, y)):\n",
    "        \n",
    "        trainX, trainY = train.iloc[trainIds], y.iloc[trainIds]\n",
    "        valX, valY = train.iloc[valIds], y.iloc[valIds]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization (from https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code)\n",
    "        clf = lgb.LGBMClassifier(\n",
    "            objective = 'binary',\n",
    "            boosting_type = 'gbdt',\n",
    "            nthread=4,\n",
    "            n_estimators=5000,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "\n",
    "        clf.fit(\n",
    "            trainX,\n",
    "            trainY,\n",
    "            eval_set=[(trainX, trainY), (valX, valY)],\n",
    "            eval_metric='auc',\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=200 \n",
    "        )\n",
    "\n",
    "        # Show validation scores                        \n",
    "        valPreds[valIds] = clf.predict_proba(valX, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        print('Fold %2d AUC : %.6f' %(fold + 1, roc_auc_score(valY, valPreds[valIds])))\n",
    "        \n",
    "        # Free space\n",
    "        del trainX, trainY, valX, valY\n",
    "        gc.collect()\n",
    "\n",
    "    \n",
    "    # Feature Importance \n",
    "\n",
    "    # Split another time to compute meaningful pimp\n",
    "    trainX, valX, trainY, valY = train_test_split(train, y, test_size = 0.25, random_state = 43)\n",
    "    \n",
    "    print('Computing features permutation importance')\n",
    "    resultAuc = permutation_importance(clf, valX, valY, scoring='roc_auc', n_repeats=2, random_state=42, n_jobs=-1)\n",
    "    importanceDf.loc[0] += (resultAuc.importances_mean / folds.n_splits) #N: same as dividing after!\n",
    "    \n",
    "    print('5 Most important features for Fold %2d' %(fold + 1))\n",
    "    best5Ids = resultAuc.importances_mean.argsort()[::-1][:5]\n",
    "    print(train.columns[best5Ids])\n",
    "\n",
    "    return roc_auc_score(y, valPreds), importanceDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training until validation scores don't improve for 200 rounds\n[100]\ttraining's auc: 0.784843\ttraining's binary_logloss: 0.240265\tvalid_1's auc: 0.765697\tvalid_1's binary_logloss: 0.244769\n[200]\ttraining's auc: 0.80784\ttraining's binary_logloss: 0.230811\tvalid_1's auc: 0.778215\tvalid_1's binary_logloss: 0.239896\n[300]\ttraining's auc: 0.821694\ttraining's binary_logloss: 0.225044\tvalid_1's auc: 0.782862\tvalid_1's binary_logloss: 0.2381\n[400]\ttraining's auc: 0.832835\ttraining's binary_logloss: 0.220492\tvalid_1's auc: 0.785045\tvalid_1's binary_logloss: 0.237187\n[500]\ttraining's auc: 0.842018\ttraining's binary_logloss: 0.216701\tvalid_1's auc: 0.786146\tvalid_1's binary_logloss: 0.236701\n[600]\ttraining's auc: 0.849901\ttraining's binary_logloss: 0.213399\tvalid_1's auc: 0.787015\tvalid_1's binary_logloss: 0.236336\n[700]\ttraining's auc: 0.857528\ttraining's binary_logloss: 0.210199\tvalid_1's auc: 0.787434\tvalid_1's binary_logloss: 0.236154\n[800]\ttraining's auc: 0.864693\ttraining's binary_logloss: 0.207043\tvalid_1's auc: 0.78772\tvalid_1's binary_logloss: 0.235994\n[900]\ttraining's auc: 0.871374\ttraining's binary_logloss: 0.204086\tvalid_1's auc: 0.787683\tvalid_1's binary_logloss: 0.235978\nEarly stopping, best iteration is:\n[782]\ttraining's auc: 0.863317\ttraining's binary_logloss: 0.207641\tvalid_1's auc: 0.787753\tvalid_1's binary_logloss: 0.236002\nFold  1 AUC : 0.787753\nTraining until validation scores don't improve for 200 rounds\n[100]\ttraining's auc: 0.784758\ttraining's binary_logloss: 0.240294\tvalid_1's auc: 0.76697\tvalid_1's binary_logloss: 0.244417\n[200]\ttraining's auc: 0.807749\ttraining's binary_logloss: 0.230804\tvalid_1's auc: 0.778961\tvalid_1's binary_logloss: 0.239426\n[300]\ttraining's auc: 0.821545\ttraining's binary_logloss: 0.22507\tvalid_1's auc: 0.783229\tvalid_1's binary_logloss: 0.237716\n[400]\ttraining's auc: 0.832865\ttraining's binary_logloss: 0.220473\tvalid_1's auc: 0.785874\tvalid_1's binary_logloss: 0.236733\n[500]\ttraining's auc: 0.84265\ttraining's binary_logloss: 0.216507\tvalid_1's auc: 0.787409\tvalid_1's binary_logloss: 0.236167\n[600]\ttraining's auc: 0.851176\ttraining's binary_logloss: 0.212968\tvalid_1's auc: 0.788162\tvalid_1's binary_logloss: 0.235857\n[700]\ttraining's auc: 0.858754\ttraining's binary_logloss: 0.209695\tvalid_1's auc: 0.788857\tvalid_1's binary_logloss: 0.235651\n[800]\ttraining's auc: 0.865989\ttraining's binary_logloss: 0.20652\tvalid_1's auc: 0.789269\tvalid_1's binary_logloss: 0.235481\n[900]\ttraining's auc: 0.872867\ttraining's binary_logloss: 0.203485\tvalid_1's auc: 0.789595\tvalid_1's binary_logloss: 0.235366\n[1000]\ttraining's auc: 0.878953\ttraining's binary_logloss: 0.200698\tvalid_1's auc: 0.789544\tvalid_1's binary_logloss: 0.235401\n[1100]\ttraining's auc: 0.884675\ttraining's binary_logloss: 0.197982\tvalid_1's auc: 0.789395\tvalid_1's binary_logloss: 0.23544\nEarly stopping, best iteration is:\n[915]\ttraining's auc: 0.873779\ttraining's binary_logloss: 0.203065\tvalid_1's auc: 0.789637\tvalid_1's binary_logloss: 0.235359\nFold  2 AUC : 0.789637\nTraining until validation scores don't improve for 200 rounds\n[100]\ttraining's auc: 0.785746\ttraining's binary_logloss: 0.238919\tvalid_1's auc: 0.764689\tvalid_1's binary_logloss: 0.24954\n[200]\ttraining's auc: 0.808257\ttraining's binary_logloss: 0.229525\tvalid_1's auc: 0.776184\tvalid_1's binary_logloss: 0.244848\n[300]\ttraining's auc: 0.822019\ttraining's binary_logloss: 0.223731\tvalid_1's auc: 0.780736\tvalid_1's binary_logloss: 0.243154\n[400]\ttraining's auc: 0.833034\ttraining's binary_logloss: 0.219192\tvalid_1's auc: 0.783083\tvalid_1's binary_logloss: 0.242351\n[500]\ttraining's auc: 0.842793\ttraining's binary_logloss: 0.215177\tvalid_1's auc: 0.784569\tvalid_1's binary_logloss: 0.241832\n[600]\ttraining's auc: 0.850889\ttraining's binary_logloss: 0.211829\tvalid_1's auc: 0.785445\tvalid_1's binary_logloss: 0.241562\n[700]\ttraining's auc: 0.858603\ttraining's binary_logloss: 0.208594\tvalid_1's auc: 0.786071\tvalid_1's binary_logloss: 0.241363\n[800]\ttraining's auc: 0.865496\ttraining's binary_logloss: 0.205563\tvalid_1's auc: 0.786425\tvalid_1's binary_logloss: 0.241291\n[900]\ttraining's auc: 0.871934\ttraining's binary_logloss: 0.202681\tvalid_1's auc: 0.786689\tvalid_1's binary_logloss: 0.241196\n[1000]\ttraining's auc: 0.878413\ttraining's binary_logloss: 0.199781\tvalid_1's auc: 0.786914\tvalid_1's binary_logloss: 0.241147\n[1100]\ttraining's auc: 0.884113\ttraining's binary_logloss: 0.197119\tvalid_1's auc: 0.787062\tvalid_1's binary_logloss: 0.241118\n[1200]\ttraining's auc: 0.889651\ttraining's binary_logloss: 0.194431\tvalid_1's auc: 0.787055\tvalid_1's binary_logloss: 0.241132\nEarly stopping, best iteration is:\n[1044]\ttraining's auc: 0.880939\ttraining's binary_logloss: 0.198588\tvalid_1's auc: 0.78711\tvalid_1's binary_logloss: 0.241084\nFold  3 AUC : 0.787110\nTraining until validation scores don't improve for 200 rounds\n[100]\ttraining's auc: 0.784377\ttraining's binary_logloss: 0.240693\tvalid_1's auc: 0.763476\tvalid_1's binary_logloss: 0.243776\n[200]\ttraining's auc: 0.807457\ttraining's binary_logloss: 0.231291\tvalid_1's auc: 0.776332\tvalid_1's binary_logloss: 0.238791\n[300]\ttraining's auc: 0.821479\ttraining's binary_logloss: 0.225479\tvalid_1's auc: 0.781523\tvalid_1's binary_logloss: 0.236804\n[400]\ttraining's auc: 0.832386\ttraining's binary_logloss: 0.220992\tvalid_1's auc: 0.78402\tvalid_1's binary_logloss: 0.235864\n[500]\ttraining's auc: 0.841695\ttraining's binary_logloss: 0.217107\tvalid_1's auc: 0.78555\tvalid_1's binary_logloss: 0.235261\n[600]\ttraining's auc: 0.849664\ttraining's binary_logloss: 0.213796\tvalid_1's auc: 0.786264\tvalid_1's binary_logloss: 0.234968\n[700]\ttraining's auc: 0.857566\ttraining's binary_logloss: 0.210473\tvalid_1's auc: 0.786914\tvalid_1's binary_logloss: 0.234726\n[800]\ttraining's auc: 0.864703\ttraining's binary_logloss: 0.207353\tvalid_1's auc: 0.787076\tvalid_1's binary_logloss: 0.234671\n[900]\ttraining's auc: 0.871248\ttraining's binary_logloss: 0.204424\tvalid_1's auc: 0.787255\tvalid_1's binary_logloss: 0.234628\n[1000]\ttraining's auc: 0.877258\ttraining's binary_logloss: 0.201654\tvalid_1's auc: 0.787329\tvalid_1's binary_logloss: 0.234602\n[1100]\ttraining's auc: 0.883098\ttraining's binary_logloss: 0.198911\tvalid_1's auc: 0.787475\tvalid_1's binary_logloss: 0.234541\n[1200]\ttraining's auc: 0.888582\ttraining's binary_logloss: 0.196258\tvalid_1's auc: 0.787434\tvalid_1's binary_logloss: 0.234562\nEarly stopping, best iteration is:\n[1079]\ttraining's auc: 0.882011\ttraining's binary_logloss: 0.199427\tvalid_1's auc: 0.787546\tvalid_1's binary_logloss: 0.234522\nFold  4 AUC : 0.787546\nTraining until validation scores don't improve for 200 rounds\n[100]\ttraining's auc: 0.784706\ttraining's binary_logloss: 0.240228\tvalid_1's auc: 0.764195\tvalid_1's binary_logloss: 0.245501\n[200]\ttraining's auc: 0.807387\ttraining's binary_logloss: 0.230875\tvalid_1's auc: 0.776633\tvalid_1's binary_logloss: 0.240597\n[300]\ttraining's auc: 0.821477\ttraining's binary_logloss: 0.225077\tvalid_1's auc: 0.781287\tvalid_1's binary_logloss: 0.238749\n[400]\ttraining's auc: 0.832562\ttraining's binary_logloss: 0.220593\tvalid_1's auc: 0.78386\tvalid_1's binary_logloss: 0.237776\n[500]\ttraining's auc: 0.841891\ttraining's binary_logloss: 0.216773\tvalid_1's auc: 0.785765\tvalid_1's binary_logloss: 0.237091\n[600]\ttraining's auc: 0.850433\ttraining's binary_logloss: 0.213197\tvalid_1's auc: 0.786654\tvalid_1's binary_logloss: 0.236751\n[700]\ttraining's auc: 0.858232\ttraining's binary_logloss: 0.209906\tvalid_1's auc: 0.78757\tvalid_1's binary_logloss: 0.236435\n[800]\ttraining's auc: 0.865006\ttraining's binary_logloss: 0.206967\tvalid_1's auc: 0.787828\tvalid_1's binary_logloss: 0.236321\n[900]\ttraining's auc: 0.871473\ttraining's binary_logloss: 0.204025\tvalid_1's auc: 0.788217\tvalid_1's binary_logloss: 0.2362\n[1000]\ttraining's auc: 0.877244\ttraining's binary_logloss: 0.20136\tvalid_1's auc: 0.788389\tvalid_1's binary_logloss: 0.236109\n[1100]\ttraining's auc: 0.882912\ttraining's binary_logloss: 0.198736\tvalid_1's auc: 0.788465\tvalid_1's binary_logloss: 0.236071\n[1200]\ttraining's auc: 0.888438\ttraining's binary_logloss: 0.196095\tvalid_1's auc: 0.788233\tvalid_1's binary_logloss: 0.236136\nEarly stopping, best iteration is:\n[1091]\ttraining's auc: 0.882474\ttraining's binary_logloss: 0.198947\tvalid_1's auc: 0.7885\tvalid_1's binary_logloss: 0.23607\nFold  5 AUC : 0.788500\nComputing features permutation importance\n5 Most important features for Fold  5\nIndex(['EXT_SOURCE_2', 'EXT_SOURCE_3', 'EXT_SOURCE_1', 'AMT_ANNUITY',\n       'PAYMENT_RATE'],\n      dtype='object')\nCPU times: user 2h 30min 45s, sys: 18.7 s, total: 2h 31min 4s\nWall time: 1h 32min 25s\n"
    }
   ],
   "source": [
    "# 5-Fold\n",
    "%time auscore, importances = trainNpredLgb(train, target, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Full AUC train score 0.788084\n"
    }
   ],
   "source": [
    "# Display Train AUC\n",
    "print('Full AUC train score %.6f' % auscore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save feats importance\n",
    "utilities.to_csv(importances, '../../datatmp/data/importances.csv')"
   ]
  }
 ]
}