{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import sklearn\n",
    "\n",
    "import gc\n",
    "\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pca to reduce feature space? collinearity? correlations? noisy features removal? Tikhonov regularization \n",
    "# train model with boost library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove collinear features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regularization is the process of introducing additional information in order to solve ill-posed problems or prevent overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#every time we are saving a csv, dtypes are lost by default. Define the following read and write function to preserve converted types in the first row to avoid another conversion after every new load.\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "def to_csv(df, path):\n",
    "    \n",
    "    dtypes = df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "    jtypes = json.dumps(dtypes)\n",
    "\n",
    "    fileName = os.path.splitext(path)\n",
    "\n",
    "    # save df as usual along with a json representation of the dictionary\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "    f = open(fileName[0]+'Types',\"w\")\n",
    "    f.write(jtypes)\n",
    "    f.close()\n",
    "\n",
    "    # free memory\n",
    "    gc.enable()\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "def read_csv(path):\n",
    "    \n",
    "    fileName = os.path.splitext(path)\n",
    "    \n",
    "    jtypes = json.load(open(fileName[0]+'Types'))\n",
    "    \n",
    "    return pd.read_csv(path, dtype=jtypes)\n",
    "\n",
    "def read_csvTmp(path):\n",
    "    \n",
    "    fileName = os.path.splitext(path)\n",
    "    \n",
    "    jtypes = json.load(open(fileName[0]+'Types'))\n",
    "    \n",
    "    return pd.read_csv(path, dtype=jtypes, nrows=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load final train\n",
    "train = read_csv('../../data/trainjoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be removed, issue fixed in _traintest\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "for c in train:\n",
    "    if train[c].dtype == 'bool':\n",
    "        le.fit(train[c])\n",
    "        train[c]=le.transform(train[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute the median over the remaining nan values\n",
    "from sklearn.impute import SimpleImputer \n",
    "imputer = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "\n",
    "imputer.fit(train)\n",
    "train.loc[:] = imputer.transform(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for null values\n",
    "nulls= train.isnull().sum()\n",
    "nulls= nulls[nullssing_values=np.n > 0]\n",
    "\n",
    "nulls.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute the upper triangle of the Pearson correlation coefficient matrix. The pearson coefficient is computed between every pair of features\n",
    "corrMatrix = train.drop('TARGET', axis=1).corr().abs()\n",
    "tableCorrelations = corrMatrix.where(np.triu(np.ones(corrMatrix.shape), k=1).astype(np.bool)).stack().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highCorr = [corr[1:] for corr in tableCorrelations.itertuples() if (corr[3] > 0.90)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "del tableCorrelations\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highCollFeat = set([corr[0] for corr in highCorr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(highCollFeat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highCollFeat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop(highCollFeat, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_csv('../../data/testjoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to be removed, issue fixed in _traintest\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "for c in test:\n",
    "    if test[c].dtype == 'bool':\n",
    "        le.fit(test[c])\n",
    "        test[c]=le.transform(test[c])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# align test and set by features as always\n",
    "\n",
    "target = train['TARGET']\n",
    "\n",
    "#Align the training and testing data, keep only columns present in both dataframes\n",
    "train, test = train.align(test, join = 'inner', axis = 1)\n",
    "\n",
    "#Add the target back in\n",
    "train['TARGET'] = target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv(train, '../../data/trainjoincoll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store collinear-free features test\n",
    "to_csv(test, '../../data/testjoincoll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read final test and train set. Remember that this dataframes are the result of merging the whole data and removing collinear features\n",
    "train = read_csv('../../data/trainjoincoll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_csvTmp('../../data/trainjoincoll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = read_csv('../../data/testjoincoll.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Feature Importance ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this notebook we employed a number of feature selection methods. These methods are necessary to reduce the number of features to increase model interpretability, decrease model runtime, and increase generalization performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Compute Pearson correlation coefficients: https://en.wikipedia.org/wiki/Pearson_correlation_coefficient. between every variable and the target\n",
    "\n",
    "def computePearson(feature):\n",
    "    corr = np.absolute(np.corrcoef(train[feature], train['TARGET'])[0,1])\n",
    "    if np.isnan(corr):\n",
    "        corr = 0\n",
    "    return (feature, corr)\n",
    "\n",
    "pcorrTarget = [ computePearson(feat) for feat in train if (feat != 'TARGET') and (feat != 'SK_ID_CURR') ]\n",
    "# Sort from descending order\n",
    "sortedPcorr = sorted(pcorrTarget, key=lambda feat:feat[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None of the features seem to be strongly correlated w/ the target w/ respect to Evans (1996) general interpretatoins (http://www.statstutor.ac.uk/resources/uploaded/pearsons.pdf). Indeed having an abs value Pearson coefficient between .00-.19 is considered as \"very week\" correlation.\n",
    "sortedPcorr[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the support of P coeff i.e. mark the features which are sligthly correlated to the target\n",
    "pcorrSupport = [True if feat[1] > 0.03 else False for feat in pcorrTarget]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pcorrSupport)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcorrSupport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As for _traintest, noisy features test. For efficiency test only on new features and Fold=8\n",
    "from featexp import get_trend_stats\n",
    "\n",
    "# select the joined new features\n",
    "newFeatsTrain = train.loc[:,'DAYS_EMPLOYED_ANOM':] \n",
    "\n",
    "# Build a validation set\n",
    "msk = np.random.rand(len(newFeatsTrain)) < 0.7\n",
    "trainset = newFeatsTrain[msk].astype(np.float32)\n",
    "validationset = newFeatsTrain[~msk].astype(np.float32)\n",
    "\n",
    "# Compute noisy's statistics for each feature wrt the target\n",
    "stats = get_trend_stats(data=trainset, target_col='TARGET', data_test=validationset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_trend_correlations=stats['Trend_correlation']\n",
    "for i in range(0,7):\n",
    "    msk = np.random.rand(len(newFeatsTrain)) < 0.7\n",
    "    trainset = newFeatsTrain[msk].astype(np.float32)\n",
    "    validationset = newFeatsTrain[~msk].astype(np.float32)\n",
    "    \n",
    "    ith_stats = get_trend_stats(data=trainset, target_col='TARGET', data_test=validationset)\n",
    "    ith_tc = ith_stats['Trend_correlation']\n",
    "    \n",
    "    total_trend_correlations += ith_tc\n",
    "\n",
    "averaged_trend_correlations = total_trend_correlations / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_trend_correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['Trend_correlation'] = averaged_trend_correlations\n",
    "# Select returned noisy feats.\n",
    "noisyFeats = stats[stats['Trend_correlation'] < 0.80]['Feature'].to_list()\n",
    "\n",
    "#drop noisy features\n",
    "train.drop(noisyFeats, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(noisyFeats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store train without noisy features\n",
    "to_csv(train, '../../data/trainjoincollnoisy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = read_csv('../../data/trainjoincollnoisy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Noisy support for the new features\n",
    "noisySupport = [False if feat in noisyFeats else True for feat in newFeatsTrain.columns if feat != 'TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "del stats\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Importance: Tree based methods\n",
    "# Tree-based models (and consequently ensembles of trees) can determine an \"importance\" for each feature by measuring the reduction in impurity for including the feature in the model. We will use a Gradient Boosted Model from the LightGBM library to assess feature importances.\n",
    "\n",
    "\n",
    "# Remove the target for training and cast to int\n",
    "target = train['TARGET'].astype(int)\n",
    "train.drop('TARGET', axis = 1, inplace=True)\n",
    "\n",
    "# An array to store feats importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportancePimp = pd.read_csv('../../data/featimp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "randomId = featureImportancePimp.index.get_loc(featureImportancePimp[featureImportancePimp['Feature'] == 'random'].index[0])\n",
    "featureImportancePimp = featureImportancePimp.iloc[:181,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainImp = train[importantFeatures]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An array to store feats importance\n",
    "featureImportance = np.zeros(train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = trainImp['TARGET'].astype(int)\n",
    "trainImp.drop('TARGET', axis = 1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model with hyperparameters similar to https://www.kaggle.com/willkoehrsen/introduction-to-feature-selection\n",
    "model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Due to the last version of lgb, we got an error related to special chars in column names. https://www.kaggle.com/c/data-science-bowl-2019/discussion/120344 commands fixed it\n",
    "trainImp.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in trainImp.columns]\n",
    "\n",
    "# Fit twice to avoid overfitting\n",
    "for i in range(2):\n",
    "    \n",
    "    # Split into training and validation set\n",
    "    trainfeats, validFeats, trainY, validY = train_test_split(trainImp, target, test_size = 0.25, random_state = i)\n",
    "    \n",
    "    # Train using early stopping\n",
    "    model.fit(trainfeats, trainY, early_stopping_rounds=80, eval_set = [(validFeats, validY)], eval_metric = 'auc', verbose = 200)\n",
    "    \n",
    "    # Record the feature importances\n",
    "    featureImportance += model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 80 rounds\n",
      "[200]\tvalid_0's auc: 0.780906\tvalid_0's binary_logloss: 0.513547\n",
      "Early stopping, best iteration is:\n",
      "[240]\tvalid_0's auc: 0.781624\tvalid_0's binary_logloss: 0.503968\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[118]\tvalid_0's auc: 0.783012\tvalid_0's binary_logloss: 0.533104\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Due to the last version of lgb, we got an error related to special chars in column names. https://www.kaggle.com/c/data-science-bowl-2019/discussion/120344 commands fixed it\n",
    "train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in train.columns]\n",
    "\n",
    "# Fit twice to avoid overfitting and Gini bias\n",
    "for i in range(2):\n",
    "    \n",
    "    # Split into training and validation set\n",
    "    trainX, validX, trainY, validY = train_test_split(train, target, test_size = 0.25, random_state = i)\n",
    "    \n",
    "    # Train using early stopping\n",
    "    model.fit(trainX, trainY, early_stopping_rounds=80, eval_set = [(validX, validY)], eval_metric = 'auc', verbose = 200)\n",
    "    \n",
    "    # Record the feature importances\n",
    "    featureImportance += model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance = featureImportance / 2\n",
    "featureImportance = pd.DataFrame({'feature': list(train.columns), 'importance': featureImportance}).sort_values('importance', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 68 features with 0.0 importance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>prev_installments_NUM_INSTALMENT_VERSION_min_min</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>OCCUPATION_TYPE_IT_staff</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>OCCUPATION_TYPE_HR_staff</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>prev_NAME_SELLER_INDUSTRY_Tourism_mean</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>469</th>\n",
       "      <td>bureau_CREDIT_TYPE_Real_estate_loan_mean</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              feature  importance\n",
       "202  prev_installments_NUM_INSTALMENT_VERSION_min_min         0.0\n",
       "93                           OCCUPATION_TYPE_IT_staff         0.0\n",
       "91                           OCCUPATION_TYPE_HR_staff         0.0\n",
       "393            prev_NAME_SELLER_INDUSTRY_Tourism_mean         0.0\n",
       "469          bureau_CREDIT_TYPE_Real_estate_loan_mean         0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the features with zero importance and remove em\n",
    "zeroFeats = featureImportance[featureImportance['importance'] == 0.0]['feature'].to_list()\n",
    "print('There are %d features with 0.0 importance' % len(zeroFeats))\n",
    "featureImportance.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportancePimp = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in featureImportancePimp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportancePimp = set(featureImportancePimp).intersection(set(train.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "174"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(featureImportancePimp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeaturesLgb = set(featureImportance['feature']).difference(set(zeroFeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "402"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(importantFeaturesLgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeatures = importantFeaturesLgb.union(featureImportancePimp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "413"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(importantFeatures)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute again the feature importance with the new dataset and check if the model gives 0 to any of the founded features\n",
    "trainFinal = train[list(importantFeatures)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportanceFinal = np.zeros(trainFinal.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 80 rounds\n",
      "[200]\tvalid_0's auc: 0.780209\tvalid_0's binary_logloss: 0.513187\n",
      "Early stopping, best iteration is:\n",
      "[157]\tvalid_0's auc: 0.780951\tvalid_0's binary_logloss: 0.52386\n",
      "Training until validation scores don't improve for 80 rounds\n",
      "Early stopping, best iteration is:\n",
      "[118]\tvalid_0's auc: 0.783012\tvalid_0's binary_logloss: 0.533104\n"
     ]
    }
   ],
   "source": [
    "# Fit twice to avoid overfitting and Gini bias\n",
    "for i in range(2):\n",
    "    \n",
    "    # Split into training and validation set\n",
    "    trainX, validX, trainY, validY = train_test_split(trainFinal, target, test_size = 0.25, random_state = i)\n",
    "    \n",
    "    # Train using early stopping\n",
    "    model.fit(trainX, trainY, early_stopping_rounds=80, eval_set = [(validX, validY)], eval_metric = 'auc', verbose = 200)\n",
    "    \n",
    "    # Record the feature importances\n",
    "    featureImportanceFinal += model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportanceFinal = featureImportanceFinal / 2\n",
    "featureImportanceFinal = pd.DataFrame({'feature': list(trainFinal.columns), 'importance': featureImportanceFinal}).sort_values('importance', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 27 features with 0.0 importance\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>ORGANIZATION_TYPE_Industry__type_1</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>ORGANIZATION_TYPE_Restaurant</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>241</th>\n",
       "      <td>prev_creditcard_AMT_DRAWINGS_ATM_CURRENT_max_sum</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>310</th>\n",
       "      <td>ORGANIZATION_TYPE_Other</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>WALLSMATERIAL_MODE_Monolithic</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              feature  importance\n",
       "34                 ORGANIZATION_TYPE_Industry__type_1         0.0\n",
       "132                      ORGANIZATION_TYPE_Restaurant         0.0\n",
       "241  prev_creditcard_AMT_DRAWINGS_ATM_CURRENT_max_sum         0.0\n",
       "310                           ORGANIZATION_TYPE_Other         0.0\n",
       "128                     WALLSMATERIAL_MODE_Monolithic         0.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the features with zero importance and remove em\n",
    "zeroFeats = featureImportanceFinal[featureImportanceFinal['importance'] == 0.0]['feature'].to_list()\n",
    "print('There are %d features with 0.0 importance' % len(zeroFeats))\n",
    "featureImportanceFinal.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "importantFeatures = set(featureImportanceFinal['feature']).difference(set(zeroFeats))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainFfinal = trainFinal[list(importantFeatures)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(307511, 386)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainFfinal.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null importance\n",
    "\n",
    "# Article: https://academic.oup.com/bioinformatics/article/26/10/1340/193348\n",
    "# As suggested in: https://www.kaggle.com/ogrellier/feature-selection-with-null-importances/output\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the target for training and cast to int\n",
    "target = train['TARGET'].astype(int)\n",
    "train.drop(['TARGET', 'SK_ID_CURR'], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features Importance and AUC scores retrival using Lgb in RF \n",
    "\n",
    "def featureImpAuc(featuresDf, target, shuffle=False):\n",
    "\n",
    "    # Due to the last version of lgb, we got an error related to special chars in column names. https://www.kaggle.com/c/              data-science-bowl-2019/discussion/120344 commands fixed it\n",
    "    featuresDf.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in featuresDf.columns]\n",
    "    \n",
    "    # Compute importance for a randomly permuted target\n",
    "    if shuffle:\n",
    "        target = np.random.permutation(target)\n",
    "    \n",
    "    # Fit the model using adapted parameters from Kaggle's notebook\n",
    "    # We will do hyper-parameters tuning => do not care too much\n",
    "    lgbParms = {\n",
    "       \n",
    "        'objective': 'binary',\n",
    "        'boosting_type': 'rf',\n",
    "        'subsample': 0.623, # bagging fraction, for each tree use a fraction of data. used to speed up training and avoid overfitting\n",
    "        'bagging_freq': 1, # apply begging at every iteration\n",
    "        'colsample_bytree': 0.8, #feature fraction, same as subsample but for features\n",
    "        'num_leaves': 127, \n",
    "        'max_depth': 8,\n",
    "        'n_jobs': 4\n",
    "    }\n",
    "\n",
    "    lgbTrain = lgb.Dataset(featuresDf, target)\n",
    "    model = lgb.train(params=lgbParms, train_set=lgbTrain, num_boost_round=200)\n",
    "    \n",
    "    # Get feature importances\n",
    "    featureImportance = pd.DataFrame({'feature': list(featuresDf.columns),\n",
    "                                      'importance_gain': model.feature_importance(importance_type='gain'),\n",
    "                                      'importance_split': model.feature_importance(importance_type='split'),\n",
    "                                      'auc_score': roc_auc_score(target, model.predict(featuresDf)) \n",
    "                                      }).sort_values('importance_gain', ascending = False)\n",
    "\n",
    "    return featureImportance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get actual importance\n",
    "featureActualImportance = featureImpAuc(train, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureActualImportance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PCA\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "data = train.drop(columns=['SK_ID_CURR', 'TARGET'])\n",
    "mmscaler = MinMaxScaler()\n",
    "X = mmscaler.fit_transform(data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca = PCA().fit(X)\n",
    "\n",
    "#Plotting the Cumulative Summation of the Explained Variance\n",
    "plt.figure()\n",
    "plt.plot(np.cumsum(X_pca.explained_variance_ratio_))\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Variance (%)') #for each component\n",
    "plt.title('Pulsar Dataset Explained Variance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('200 principal components account for {:.4f}% of the variance.'.format(100 * np.sum(X_pca.explained_variance_ratio_[:200])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=200)\n",
    "trainPca=pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Pca = pd.DataFrame(trainPca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Pca['SK_ID_CURR'] = train['SK_ID_CURR']\n",
    "train_Pca['TARGET'] = train['TARGET']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Pca['TARGET'] = train_Pca['TARGET'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_Pca['TARGET']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Pca = train_Pca.drop('TARGET', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_Pca = train_Pca.drop('SK_ID_CURR', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "del train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_importances = np.zeros(train_Pca.shape[1])\n",
    "\n",
    "# Create the model with several hyperparameters\n",
    "model = lgb.LGBMClassifier(objective='binary', boosting_type = 'goss', n_estimators = 10000, class_weight = 'balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i in range(2):\n",
    "    \n",
    "    # Split into training and validation set\n",
    "    train_features, valid_features, train_y, valid_y = train_test_split(train_Pca, target, test_size = 0.25, random_state = i)\n",
    "    \n",
    "    # Train using early stopping\n",
    "    model.fit(train_features, train_y, early_stopping_rounds=100, eval_set = [(valid_features, valid_y)], \n",
    "              eval_metric = 'auc', verbose = 200)\n",
    "    \n",
    "    # Record the feature importances\n",
    "    feature_importances += model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to average feature importances! \n",
    "feature_importances = feature_importances / 2\n",
    "feature_importances = pd.DataFrame({'feature': list(train_Pca.columns), 'importance': feature_importances}).sort_values('importance', ascending = False)\n",
    "\n",
    "feature_importances.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_features = list(feature_importances[feature_importances['importance'] == 0.0]['feature'])\n",
    "print('There are %d features with 0.0 importance' % len(zero_features))\n",
    "feature_importances.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrSupport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrTargetList[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corrTargetList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize w/ a density plot how the younger clients tend to default more likely\n",
    "#plt.figure(figsize = (10, 8))\n",
    "#\n",
    "## KDE plot of loans that were repaid on time\n",
    "#sns.kdeplot(train.loc[train['TARGET'] == 0, 'DAYS_BIRTH'] / -365, label = 'target == 0')\n",
    "#\n",
    "## KDE plot of loans which were not repaid on time\n",
    "#sns.kdeplot(train.loc[train['TARGET'] == 1, 'DAYS_BIRTH'] / -365, label = 'target == 1')\n",
    "## Labeling of plot\n",
    "#plt.xlabel('Age (years)'); plt.ylabel('Density'); plt.title('Distribution of Ages');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the null importances distributions : these are created fitting the model over several runs on a shuffled version of the target. This shows how the model can make sense of a feature irrespective of the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### NULL IMPORTANCE ###\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import KFold\n",
    "import time\n",
    "from lightgbm import LGBMClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We show that this method can be used to correct for the bias of feature importance computed with RF and MI. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all features w/ 0 overall importance using: Person Coefficient, Chi coefficient, Logistic regression and RF w/ lightGBM.\n"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
