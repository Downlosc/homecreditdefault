{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shrink down as much as we can the size of the dataframes.\n",
    "#Note that every numerical value lies within the range indexed with a float/int of 32 bits\n",
    "#Moreover is wise to convert every object feature into a category one, especially if the number of unique values is far from the number of rows\n",
    "\n",
    "import sys\n",
    "\n",
    "def convert_types(df, print_info = False):\n",
    "    \n",
    "    original_memory = df.memory_usage().sum()\n",
    "    \n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "        \n",
    "        # Convert ids to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object'):\n",
    "            df[c] = df[c].astype('category')\n",
    "        \n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    new_memory = df.memory_usage().sum()\n",
    "    \n",
    "    if print_info:\n",
    "        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n",
    "        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load preprocessed train and convert types and sanity check\n",
    "train=pd.read_csv('../../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.46 gb.\nNew Memory Usage: 0.23 gb.\n"
    }
   ],
   "source": [
    "train=convert_types(train, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOTIVATION: We are not loan's domain experts, thus ... (see notes on ipad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def onehot_binenc(df, type = 'object'):\n",
    "    \"\"\" dtype should be 'object' or 'category' depending on the dataframe being converted or not \"\"\"\n",
    "    \n",
    "    le = sklearn.preprocessing.LabelEncoder()\n",
    "    #counter for binary categorical features\n",
    "    bcount = 0\n",
    "    #find feaures w two categories and transform them either to 0 or 1\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == type and len(df[col].unique()) <= 2 :\n",
    "            le.fit(df[col])\n",
    "            df[col]=le.transform(df[col])\n",
    "            bcount+=1\n",
    "    \n",
    "    #one hot encoding of the remaining k-categorical features, w/ k>2. If there's any\n",
    "    if (bcount < df.shape[1]):\n",
    "        df = pd.get_dummies(df)\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4. Merging ####\n",
    "#define a function to left join two datasets by handling separately numerical features and categorical ones\n",
    "def join_w_stats(id, df1, df2, df2_name):\n",
    "    \"\"\" Merge two dataframes (df1 and df2) by grouping df2 on id and computing the following statistics:\n",
    "            i) Mean, Min and Max and sum for numeric features\n",
    "            ii) Mean for categorical features \n",
    "        In this way, indeed, we hope to preserve the essence of the information stored in each feature after groub by\"\"\"\n",
    "\n",
    "    #drop from df2 the id column since it is not necessary and won't be used anymore\n",
    "    df2 = df2.drop([col for col in df2.columns if col.startswith('SK_ID') and col != id], axis=1)\n",
    "    newcolumns = []\n",
    "    \n",
    "    \n",
    "    #compute statistics for numerical feats, if there's any\n",
    "    numericaldf2 = df2.select_dtypes(include='number')\n",
    "    count_numericalcols = len(numericaldf2.columns)\n",
    "    if count_numericalcols > 1: #1 is the id\n",
    "        \n",
    "        numericaldf2[id] = df2[id]\n",
    "        numstatsdf2 = numericaldf2.groupby(id).agg(['mean', 'max', 'min', 'sum']).reset_index()\n",
    "\n",
    "        #create new columns names for each numerical feature_stat\n",
    "        for col in numstatsdf2.columns.levels[0]: \n",
    "            if col != id:\n",
    "                #loop through every subcolumn name\n",
    "                for stat in numstatsdf2.columns.levels[1][:-1]:\n",
    "                    newcolumns.append('%s_%s_%s' % (df2_name, col, stat))\n",
    "\n",
    "   \n",
    "    #compute mean for categorical feats, if there's any\n",
    "    categorical = False\n",
    "    if (len(df2.columns) - count_numericalcols) > 0:\n",
    "        categoricaldf2 = df2.select_dtypes(include='category')\n",
    "        categorical = True\n",
    "        onehotdf2 = onehot_binenc(categoricaldf2, 'category')\n",
    "        onehotdf2[id] = df2[id]\n",
    "        onehotstatsdf2 = onehotdf2.groupby(id).agg(['mean']).reset_index()\n",
    "    \n",
    "        #create new columns names for each categorical feature_stat\n",
    "        for col in onehotstatsdf2.columns.levels[0]: \n",
    "            if col != id:\n",
    "                #for categoricals the only subcolumn is the mean\n",
    "                newcolumns.append('%s_%s_mean' % (df2_name, col))\n",
    "\n",
    "    # df2 no longer needed. Free memory\n",
    "    gc.enable()\n",
    "    del df2\n",
    "    gc.collect()\n",
    "\n",
    "    #merge both numerical and categorical (if there is any) statistics dsets grouped by id. And then with df1\n",
    "    if categorical == True:\n",
    "        numstatsdf2 = numstatsdf2.join(onehotstatsdf2.set_index(id), on=id)\n",
    "        \n",
    "    #add new columns names    \n",
    "    numstatsdf2.columns = [id]+newcolumns \n",
    "    #left join on id df1 w/ merged statistics of df2\n",
    "    df1joindf2 = df1.join(numstatsdf2.set_index(id), on=id)\n",
    "\n",
    "\n",
    "    # df1 no longer needed. Free memory\n",
    "    gc.enable()\n",
    "    del df1\n",
    "    gc.collect()\n",
    "\n",
    "    # some cast might happen during merge and groupby, thus convert again\n",
    "    convert_types(df1joindf2)\n",
    "\n",
    "    return df1joindf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "previousApplication = pd.read_csv('../../data/previous_application.csv')\n",
    "installmentsPayments = pd.read_csv('../../data/installments_payments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.49 gb.\nNew Memory Usage: 0.17 gb.\n"
    }
   ],
   "source": [
    "previousApplication = convert_types(previousApplication, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.87 gb.\nNew Memory Usage: 0.44 gb.\n"
    }
   ],
   "source": [
    "installmentsPayments = convert_types(installmentsPayments, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "previousJOINinstallments = join_w_stats('SK_ID_PREV', previousApplication, installmentsPayments, 'installments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "cashBalance = pd.read_csv('../../data/POS_CASH_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.64 gb.\nNew Memory Usage: 0.29 gb.\n"
    }
   ],
   "source": [
    "cashBalance = convert_types(cashBalance, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
    }
   ],
   "source": [
    "previousJOINcashBalanceJOINinstallments = join_w_stats('SK_ID_PREV', previousJOINinstallments, cashBalance, 'cash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditCardBalance = pd.read_csv('../../data/credit_card_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.71 gb.\nNew Memory Usage: 0.34 gb.\n"
    }
   ],
   "source": [
    "creditCardBalance = convert_types(creditCardBalance, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
    }
   ],
   "source": [
    "previousJOINcashBalanceJOINinstallmentsJOINcreditCardBalance = join_w_stats('SK_ID_PREV', previousJOINcashBalanceJOINinstallments, creditCardBalance, 'creditcard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#every time we are saving a csv, dtypes are lost by default. Define the following read and write function to preserve converted types in the first row to avoid another conversion after every loading.\n",
    "\n",
    "import os\n",
    "import json\n",
    "\n",
    "def to_csv(df, path):\n",
    "    \n",
    "    dtypes = df.dtypes.apply(lambda x: x.name).to_dict()\n",
    "    jtypes = json.dumps(dtypes)\n",
    "\n",
    "    fileName = os.path.splitext(path)\n",
    "\n",
    "    # save df as usual along with a json representation of the dictionary\n",
    "    df.to_csv(path, index=False)\n",
    "\n",
    "    f = open(fileName[0]+'Types',\"w\")\n",
    "    f.write(jtypes)\n",
    "    f.close()\n",
    "\n",
    "    # free memory\n",
    "    gc.enable()\n",
    "    del df\n",
    "    gc.collect()\n",
    "\n",
    "def read_csv(path):\n",
    "    \n",
    "    fileName = os.path.splitext(path)\n",
    "    \n",
    "    jtypes = json.load(open(fileName[0]+'Types'))\n",
    "    \n",
    "    return pd.read_csv(path, dtype=jtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/home/arbiterelegantiae/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  del sys.path[0]\n"
    }
   ],
   "source": [
    "trainJOINprev = join_w_stats('SK_ID_CURR', train, previousJOINcashBalanceJOINinstallmentsJOINcreditCardBalance, 'prev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store joined previous loans dara since it will be merged to test as well\n",
    "to_csv(previousJOINcashBalanceJOINinstallmentsJOINcreditCardBalance, '../../data/previousJoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all features with more than 60% of N.a.N\n",
    "def remove_missing_columns(df, threshold = 60):\n",
    "    # Calculate missing stats for df (remember to calculate a percent!)\n",
    "    df_miss = pd.DataFrame(df.isnull().sum())\n",
    "    df_miss['percent'] = 100 * df_miss[0] / len(df)\n",
    "    \n",
    "    \n",
    "    # list of missing columns for df\n",
    "    missing_df_columns = list(df_miss.index[df_miss['percent'] > threshold])\n",
    "    \n",
    "    # Print information\n",
    "    print('There are %d columns with greater than %d%% missing values.' % (len(missing_df_columns), threshold))\n",
    "    \n",
    "    # Drop the missing columns and return\n",
    "    df = df.drop(columns = missing_df_columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "There are 267 columns with greater than 60% missing values.\n"
    }
   ],
   "source": [
    "#drop all the new computed features that we consider no-influent from trainJOINprev\n",
    "trainJOINprev = remove_missing_columns(trainJOINprev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureauBalance = pd.read_csv('../../data/bureau_balance.csv')\n",
    "bureau = pd.read_csv('../../data/bureau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.66 gb.\nNew Memory Usage: 0.25 gb.\nOriginal Memory Usage: 0.23 gb.\nNew Memory Usage: 0.1 gb.\n"
    }
   ],
   "source": [
    "bureauBalance = convert_types(bureauBalance, print_info=True)\n",
    "bureau = convert_types(bureau, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
    }
   ],
   "source": [
    "bureauJOINbureauBalance = join_w_stats('SK_ID_BUREAU', bureau, bureauBalance, 'bureauBalance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
    }
   ],
   "source": [
    "trainJoined = join_w_stats('SK_ID_CURR', trainJOINprev, bureauJOINbureauBalance, 'bureau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "There are 39 columns with greater than 60% missing values.\n"
    }
   ],
   "source": [
    "trainJoined = remove_missing_columns(trainJoined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv(bureauJOINbureauBalance, '../../data/bureauJoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the final train\n",
    "to_csv(trainJoined, '../../data/trainjoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.07 gb.\nNew Memory Usage: 0.04 gb.\n"
    }
   ],
   "source": [
    "## apply the same logic to the test ##\n",
    "test = pd.read_csv('../../data/test.csv')\n",
    "test = convert_types(test, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously stored, read infering the right types\n",
    "prevJoined = read_csv('../../data/previousJoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/home/arbiterelegantiae/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  del sys.path[0]\n"
    }
   ],
   "source": [
    "testJOINprev = join_w_stats('SK_ID_CURR', test, prevJoined, 'prev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "There are 267 columns with greater than 60% missing values.\n"
    }
   ],
   "source": [
    "testJOINprev = remove_missing_columns(testJOINprev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureauJOINbureauBalance = read_csv('../../data/bureauJoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
    }
   ],
   "source": [
    "testJoined = join_w_stats('SK_ID_CURR', testJOINprev, bureauJOINbureauBalance, 'bureau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "There are 0 columns with greater than 60% missing values.\n"
    }
   ],
   "source": [
    "testJoined = remove_missing_columns(testJoined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainJoined = read_csv('../../data/trainjoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "778\n816\n"
    }
   ],
   "source": [
    "## it seems like there were more sparse features in train than in test that were removed, this is reasonable due to the larger ids in train\n",
    "print(len(trainJoined.columns))\n",
    "print(len(testJoined.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(307511, 778)"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to align as we did in homedefault_traintest\n",
    "\n",
    "target = trainJoined['TARGET']\n",
    "\n",
    "#Align the training and testing data, keep only columns present in both dataframes\n",
    "trainJoined, testJoined = trainJoined.align(testJoined, join = 'inner', axis = 1)\n",
    "\n",
    "#Add the target back in\n",
    "trainJoined['TARGET'] = target\n",
    "\n",
    "trainJoined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(48744, 777)"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testJoined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv(trainJoined, '../../data/trainjoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_csv(testJoined, '../../data/testjoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue to the feat engineering phase"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}