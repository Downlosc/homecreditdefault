{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load preprocessed train\n",
    "train=pd.read_csv('../../data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOTIVATION: We are not loan's domain experts, thus ... (see notes on ipad)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def onehot_binenc(df, type = 'object'):\n",
    "    \"\"\" dtype should be 'object' or 'category' depending on the dataframe being converted or not \"\"\"\n",
    "    \n",
    "    le = sklearn.preprocessing.LabelEncoder()\n",
    "    #counter for binary categorical features\n",
    "    bcount = 0\n",
    "    #find feaures w two categories and transform them either to 0 or 1\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == type and len(df[col].unique()) <= 2 :\n",
    "            le.fit(df[col])\n",
    "            df[col]=le.transform(df[col])\n",
    "            bcount+=1\n",
    "    \n",
    "    #one hot encoding of the remaining k-categorical features, w/ k>2. If there's any\n",
    "    if (bcount < df.shape[1]):\n",
    "        df = pd.get_dummies(df)\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 4. Merging ####\n",
    "#define a function to left join two datasets by handling separately numerical features and categorical ones\n",
    "def join_w_stats(id, df1, df2, df2_name):\n",
    "    \"\"\" Merge two dataframes (df1 and df2) by grouping df2 on id and computing the following statistics:\n",
    "            i) Mean, Min and Max and sum for numeric features\n",
    "            ii) Mean for categorical features \n",
    "        In this way, indeed, we hope to preserve the essence of the information stored in each feature after groub by\"\"\"\n",
    "\n",
    "    #drop from df2 the id column since it is not necessary and won't be used anymore\n",
    "    df2 = df2.drop([col for col in df2.columns if col.startswith('SK_ID') and col != id], axis=1)\n",
    "    newcolumns = []\n",
    "    \n",
    "    \n",
    "    #compute statistics for numerical feats, if there's any\n",
    "    numericaldf2 = df2.select_dtypes(include='number')\n",
    "    count_numericalcols = len(numericaldf2.columns)\n",
    "    if count_numericalcols > 1: #1 is the id\n",
    "        \n",
    "        numericaldf2[id] = df2[id]\n",
    "        numstatsdf2 = numericaldf2.groupby(id).agg(['mean', 'max', 'min', 'sum']).reset_index()\n",
    "\n",
    "        #create new columns names for each numerical feature_stat\n",
    "        for col in numstatsdf2.columns.levels[0]: \n",
    "            if col != id:\n",
    "                #loop through every subcolumn name\n",
    "                for stat in numstatsdf2.columns.levels[1][:-1]:\n",
    "                    newcolumns.append('%s_%s_%s' % (df2_name, col, stat))\n",
    "\n",
    "   \n",
    "    #compute mean for categorical feats, if there's any\n",
    "    categorical = False\n",
    "    if (len(df2.columns) - count_numericalcols) > 0:\n",
    "        categoricaldf2 = df2.select_dtypes(include='category')\n",
    "        categorical = True\n",
    "        onehotdf2 = onehot_binenc(categoricaldf2, 'category')\n",
    "        onehotdf2[id] = df2[id]\n",
    "        onehotstatsdf2 = onehotdf2.groupby(id).agg(['mean']).reset_index()\n",
    "    \n",
    "        #create new columns names for each categorical feature_stat\n",
    "        for col in onehotstatsdf2.columns.levels[0]: \n",
    "            if col != id:\n",
    "                #for categoricals the only subcolumn is the mean\n",
    "                newcolumns.append('%s_%s_mean' % (df2_name, col))\n",
    "\n",
    "\n",
    "    #merge both numerical and categorical (if there is any) statistics dsets grouped by id. And then with df1\n",
    "    if categorical == True:\n",
    "        numstatsdf2 = numstatsdf2.join(onehotstatsdf2.set_index(id), on=id)\n",
    "        \n",
    "    #add new columns names    \n",
    "    numstatsdf2.columns = [id]+newcolumns \n",
    "    #left join on id df1 w/ merged statistics of df2\n",
    "    df1joindf2 = df1.join(numstatsdf2.set_index(id), on=id)\n",
    "\n",
    "\n",
    "    gc.enable()\n",
    "    del df1, df2\n",
    "    gc.collect()\n",
    "\n",
    "\n",
    "    return df1joindf2"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Shrink down as much as we can the size of the dataframes.\n",
    "#Note that every numerical value lies within the range indexed with a float/int of 32 bits\n",
    "#Moreover is wise to convert every object feature into a category one, especially if the number of unique values is far from the number of rows\n",
    "\n",
    "import sys\n",
    "\n",
    "def convert_types(df, print_info = False):\n",
    "    \n",
    "    original_memory = df.memory_usage().sum()\n",
    "    \n",
    "    # Iterate through each column\n",
    "    for c in df:\n",
    "        \n",
    "        # Convert ids and booleans to integers\n",
    "        if ('SK_ID' in c):\n",
    "            df[c] = df[c].fillna(0).astype(np.int32)\n",
    "            \n",
    "        # Convert objects to category\n",
    "        elif (df[c].dtype == 'object') and (df[c].nunique() < df.shape[0]):\n",
    "            df[c] = df[c].astype('category')\n",
    "        \n",
    "        # Booleans mapped to integers\n",
    "        elif list(df[c].unique()) == [1, 0]:\n",
    "            df[c] = df[c].astype(bool)\n",
    "        \n",
    "        # Float64 to float32\n",
    "        elif df[c].dtype == float:\n",
    "            df[c] = df[c].astype(np.float32)\n",
    "            \n",
    "        # Int64 to int32\n",
    "        elif df[c].dtype == int:\n",
    "            df[c] = df[c].astype(np.int32)\n",
    "        \n",
    "    new_memory = df.memory_usage().sum()\n",
    "    \n",
    "    if print_info:\n",
    "        print(f'Original Memory Usage: {round(original_memory / 1e9, 2)} gb.')\n",
    "        print(f'New Memory Usage: {round(new_memory / 1e9, 2)} gb.')\n",
    "        \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "previousApplication = pd.read_csv('../../data/previous_application.csv')\n",
    "installmentsPayments = pd.read_csv('../../data/installments_payments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "previousApplication = convert_types(previousApplication, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "installmentsPayments = convert_types(installmentsPayments, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "previousJOINcashBalance = join_w_stats('SK_ID_PREV', previousApplication, installmentsPayments, 'installments') # SHOULD BE named previousJOINinstallments"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# Remove variables to free memory\n",
    "gc.enable()\n",
    "del previousApplication, installmentsPayments \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "cashBalance = pd.read_csv('../../data/POS_CASH_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "cashBalance = convert_types(cashBalance, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "previousJOINcashBalanceJOINinstallments = join_w_stats('SK_ID_PREV', previousJOINcashBalance, cashBalance, 'cash')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "# Remove variables to free memory\n",
    "gc.enable()\n",
    "del previousJOINcashBalance, cashBalance\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "creditCardBalance = pd.read_csv('../../data/credit_card_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "creditCardBalance.head()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "creditCardBalance = convert_types(creditCardBalance, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "previousJOINcashBalanceJOINinstallmentsJOINcreditCardBalance = join_w_stats('SK_ID_PREV', previousJOINcashBalanceJOINinstallments, creditCardBalance, 'creditcard')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "del previousJOINcashBalanceJOINinstallments, creditCardBalance\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "previousJOINcashBalanceJOINinstallmentsJOINcreditCardBalance.shape"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store joined previous loans dara since it will be merged to test as well\n",
    "previousJOINcashBalanceJOINinstallmentsJOINcreditCardBalance.to_csv('../../data/previousJoined.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainJOINprev = join_w_stats('SK_ID_CURR', train, previousJOINcashBalanceJOINinstallmentsJOINcreditCardBalance, 'prev')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "del previousJOINcashBalanceJOINinstallmentsJOINcreditCardBalance, train\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all features with more than 60% of N.a.N\n",
    "def remove_missing_columns(df, threshold = 60):\n",
    "    # Calculate missing stats for df (remember to calculate a percent!)\n",
    "    df_miss = pd.DataFrame(df.isnull().sum())\n",
    "    df_miss['percent'] = 100 * df_miss[0] / len(df)\n",
    "    \n",
    "    \n",
    "    # list of missing columns for df\n",
    "    missing_df_columns = list(df_miss.index[df_miss['percent'] > threshold])\n",
    "    \n",
    "    # Print information\n",
    "    print('There are %d columns with greater than %d%% missing values.' % (len(missing_df_columns), threshold))\n",
    "    \n",
    "    # Drop the missing columns and return\n",
    "    df = df.drop(columns = missing_df_columns)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop all the new computed features that we consider no-influent from trainJOINprev\n",
    "trainJOINprev = remove_missing_columns(trainJOINprev)\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureauBalance = pd.read_csv('../../data/bureau_balance.csv')\n",
    "bureau = pd.read_csv('../../data/bureau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureauBalance = convert_types(bureauBalance, print_info=True)\n",
    "bureau = convert_types(bureau, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureauJOINbureauBalance = join_w_stats('SK_ID_BUREAU', bureau, bureauBalance, 'bureauBalance')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "gc.enable()\n",
    "del bureau, bureauBalance\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainJoined = join_w_stats('SK_ID_CURR', trainJOINprev, bureauJOINbureauBalance, 'bureau')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "bureauJOINbureauBalance.to_csv('../../data/bureauJoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "del trainJOINprev, bureauJOINbureauBalance\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainJoined = remove_missing_columns(trainJoined)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the final train\n",
    "trainJoined.to_csv('../../data/trainjoined.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "## apply the same logic to the test ##\n",
    "test = pd.read_csv('../../data/test.csv')\n",
    "test = convert_types(test, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevJoined = pd.read_csv('../../data/previousJoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "#every time we are reading, dtypes are lazily computed by pandas, this means that what we converted to category would be object again, the same for numericals, thus convert again.\n",
    "prevJoined = convert_types(prevJoined, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "testJOINprev = join_w_stats('SK_ID_CURR', test, prevJoined, 'prev')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "testJOINprev = remove_missing_columns(testJOINprev)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# As before, as we are reading we need to convert the types\n",
    "bureauJOINbureauBalance = pd.read_csv('../../data/bureauJoined.csv')\n",
    "bureauJOINbureauBalance = convert_types(bureauJOINbureauBalance, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "testJoined = join_w_stats('SK_ID_CURR', testJOINprev, bureauJOINbureauBalance, 'bureau')"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "testJoined = remove_missing_columns(testJoined)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainJoined = pd.read_csv('../../data/trainjoined.csv')\n",
    "trainJoined = convert_types(trainJoined, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "## it seems like there were more sparse features in train than in shape, this is reasonable due to the larger ids in train\n",
    "trainJoined.shape\n",
    "testJoined.shape"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to align as we did in homedefault_traintest\n",
    "\n",
    "target = trainJoined['TARGET']\n",
    "\n",
    "#Align the training and testing data, keep only columns present in both dataframes\n",
    "trainJoined, testJoined = trainJoined.align(testJoined, join = 'inner', axis = 1)\n",
    "\n",
    "#Add the target back in\n",
    "trainJoined['TARGET'] = target\n",
    "\n",
    "trainJoined.shape\n",
    "testJoined.shape"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainJoined.to_csv('trainjoined.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.enable()\n",
    "del trainJoined\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "testJoined.to_csv('testjoined.csv', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue to the feat engineering phase"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}