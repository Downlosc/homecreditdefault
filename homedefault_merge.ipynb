{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load preprocessed train and convert types and sanity check\n",
    "train=pd.read_csv('../../datatmp/data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(307511, 162)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.4 gb.\nNew Memory Usage: 0.2 gb.\n"
    }
   ],
   "source": [
    "# Shrink down size by converting types\n",
    "train=utilities.convert_types(train, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We are not loan's domain experts, therefore we don't speculate too much on which features are salient or not without looking at correlations. Note that it would also be extremly time consuming given the amount of variables we are dealing with. Indeed, we will collect as many features as possible by merging all datasets and, applying general features reduction techniques and feature importance, we will let our models learn on such data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "def onehot_binenc(df, type = 'object'):\n",
    "    \"\"\" dtype should be 'object' or 'category' depending on the dataframe being converted or not \"\"\"\n",
    "    \n",
    "    le = sklearn.preprocessing.LabelEncoder()\n",
    "    #counter for binary categorical features\n",
    "    bcount = 0\n",
    "    #find feaures w two categories and transform them either to 0 or 1\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == type and len(df[col].unique()) <= 2 :\n",
    "            le.fit(df[col])\n",
    "            df[col]=le.transform(df[col])\n",
    "            bcount+=1\n",
    "    \n",
    "    #one hot encoding of the remaining k-categorical features, w/ k>2. If there's any\n",
    "    if (bcount < df.shape[1]):\n",
    "        df = pd.get_dummies(df)\n",
    "    return df\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Merging ##\n",
    "#define a function to left join two datasets by handling separately numerical features and categorical ones\n",
    "def join_w_stats(id, df1, df2, df2_name):\n",
    "    \"\"\" Merge two dataframes (df1 and df2) by grouping df2 on id and computing the following statistics:\n",
    "            i) Mean, Min and Max and sum for numeric features\n",
    "            ii) Mean for categorical features \n",
    "        In this way, indeed, we hope to preserve the essence of the information stored in each feature after groub by\"\"\"\n",
    "\n",
    "    #drop from df2 the id column since it is not necessary and won't be used anymore\n",
    "    df2 = df2.drop([col for col in df2.columns if col.startswith('SK_ID') and col != id], axis=1)\n",
    "    newcolumns = []\n",
    "    \n",
    "    \n",
    "    #compute statistics for numerical feats, if there's any\n",
    "    numericaldf2 = df2.select_dtypes(include='number')\n",
    "    count_numericalcols = len(numericaldf2.columns)\n",
    "    if count_numericalcols > 1: #1 is the id\n",
    "        \n",
    "        numericaldf2[id] = df2[id]\n",
    "        numstatsdf2 = numericaldf2.groupby(id).agg(['mean', 'max', 'min', 'sum']).reset_index()\n",
    "\n",
    "        #create new columns names for each numerical feature_stat\n",
    "        for col in numstatsdf2.columns.levels[0]: \n",
    "            if col != id:\n",
    "                #loop through every subcolumn name\n",
    "                for stat in numstatsdf2.columns.levels[1][:-1]:\n",
    "                    newcolumns.append('%s_%s_%s' % (df2_name, col, stat))\n",
    "\n",
    "   \n",
    "    #compute mean for categorical feats, if there's any\n",
    "    categorical = False\n",
    "    if (len(df2.columns) - count_numericalcols) > 0:\n",
    "        categoricaldf2 = df2.select_dtypes(include='category')\n",
    "        categorical = True\n",
    "        onehotdf2 = onehot_binenc(categoricaldf2, 'category')\n",
    "        onehotdf2[id] = df2[id]\n",
    "        onehotstatsdf2 = onehotdf2.groupby(id).agg(['mean']).reset_index()\n",
    "    \n",
    "        #create new columns names for each categorical feature_stat\n",
    "        for col in onehotstatsdf2.columns.levels[0]: \n",
    "            if col != id:\n",
    "                #for categoricals the only subcolumn is the mean\n",
    "                newcolumns.append('%s_%s_mean' % (df2_name, col))\n",
    "\n",
    "    # df2 no longer needed. Free memory\n",
    "    gc.enable()\n",
    "    del df2\n",
    "    gc.collect()\n",
    "\n",
    "    #merge both numerical and categorical (if there is any) statistics dsets grouped by id. And then with df1\n",
    "    if categorical == True:\n",
    "        numstatsdf2 = numstatsdf2.join(onehotstatsdf2.set_index(id), on=id)\n",
    "        \n",
    "    #add new columns names    \n",
    "    numstatsdf2.columns = [id]+newcolumns \n",
    "    #left join on id df1 w/ merged statistics of df2\n",
    "    df1joindf2 = df1.join(numstatsdf2.set_index(id), on=id)\n",
    "\n",
    "\n",
    "    # df1 no longer needed. Free memory\n",
    "    gc.enable()\n",
    "    del df1\n",
    "    gc.collect()\n",
    "\n",
    "    # some cast might happen during merge and groupby, thus convert again\n",
    "    utilities.convert_types(df1joindf2)\n",
    "\n",
    "    return df1joindf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "previousApplication = pd.read_csv('../../data/previous_application.csv')\n",
    "installmentsPayments = pd.read_csv('../../data/installments_payments.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.49 gb.\nNew Memory Usage: 0.17 gb.\n"
    }
   ],
   "source": [
    "previousApplication = utilities.convert_types(previousApplication, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: the same anom outlier of DAYS_EMPLOYED occurs in previousapplication as well! handle anomalies in the same way\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = sklearn.preprocessing.LabelEncoder()\n",
    "\n",
    "previousApplication['DAYS_FIRST_DRAWING_ANOM'] = previousApplication[\"DAYS_FIRST_DRAWING\"] == 365243\n",
    "previousApplication['DAYS_FIRST_DRAWING_ANOM']=le.fit_transform(previousApplication['DAYS_FIRST_DRAWING_ANOM'])\n",
    "#Replace the anomalous values with median\n",
    "previousApplication['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "previousApplication['DAYS_FIRST_DRAWING'].fillna(previousApplication['DAYS_FIRST_DRAWING'].median(), inplace=True)\n",
    "\n",
    "previousApplication['DAYS_FIRST_DUE_ANOM'] = previousApplication[\"DAYS_FIRST_DUE\"] == 365243\n",
    "previousApplication['DAYS_FIRST_DUE_ANOM']=le.fit_transform(previousApplication['DAYS_FIRST_DUE_ANOM'])\n",
    "previousApplication['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "previousApplication['DAYS_FIRST_DUE'].fillna(previousApplication['DAYS_FIRST_DUE'].median(), inplace=True)\n",
    "\n",
    "previousApplication['DAYS_LAST_DUE_1ST_VERSION_ANOM'] = previousApplication[\"DAYS_LAST_DUE_1ST_VERSION\"] == 365243\n",
    "previousApplication['DAYS_LAST_DUE_1ST_VERSION_ANOM']=le.fit_transform(previousApplication['DAYS_LAST_DUE_1ST_VERSION_ANOM'])\n",
    "previousApplication['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "previousApplication['DAYS_LAST_DUE_1ST_VERSION'].fillna(previousApplication['DAYS_LAST_DUE_1ST_VERSION'].median(), inplace=True)\n",
    "\n",
    "previousApplication['DAYS_LAST_DUE_ANOM'] = previousApplication[\"DAYS_LAST_DUE\"] == 365243\n",
    "previousApplication['DAYS_LAST_DUE_ANOM']=le.fit_transform(previousApplication['DAYS_LAST_DUE_ANOM'])\n",
    "previousApplication['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "previousApplication['DAYS_LAST_DUE'].fillna(previousApplication['DAYS_LAST_DUE'].median(), inplace=True)\n",
    "\n",
    "previousApplication['DAYS_TERMINATION_ANOM'] = previousApplication[\"DAYS_TERMINATION\"] == 365243\n",
    "previousApplication['DAYS_TERMINATION_ANOM']=le.fit_transform(previousApplication['DAYS_TERMINATION_ANOM'])\n",
    "previousApplication['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "previousApplication['DAYS_TERMINATION'].fillna(previousApplication['DAYS_TERMINATION'].median(), inplace=True)\n",
    "\n",
    "\n",
    "# from https://www.kaggle.com/jsaguiar/lightgbm-with-simple-features\n",
    "# Add feature: value ask / value received percentage\n",
    "# Note: it does create inf values\n",
    "previousApplication['APP_CREDIT_PERC'] = previousApplication['AMT_APPLICATION'] / previousApplication['AMT_CREDIT']\n",
    "# Replace inf values with nan\n",
    "previousApplication['APP_CREDIT_PERC'].replace([np.inf, -np.inf], np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.44 gb.\nNew Memory Usage: 0.44 gb.\n"
    }
   ],
   "source": [
    "installmentsPayments = utilities.convert_types(installmentsPayments, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "previousJOINinstallments = join_w_stats('SK_ID_PREV', previousApplication, installmentsPayments, 'installments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cashBalance = pd.read_csv('../../data/POS_CASH_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.64 gb.\nNew Memory Usage: 0.29 gb.\n"
    }
   ],
   "source": [
    "cashBalance = utilities.convert_types(cashBalance, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/envs/homedefaul/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
    }
   ],
   "source": [
    "previousJOINcashBalanceJOINinstallments = join_w_stats('SK_ID_PREV', previousJOINinstallments, cashBalance, 'cash')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "creditCardBalance = pd.read_csv('../../data/credit_card_balance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.71 gb.\nNew Memory Usage: 0.34 gb.\n"
    }
   ],
   "source": [
    "creditCardBalance = utilities.convert_types(creditCardBalance, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/envs/homedefaul/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
    }
   ],
   "source": [
    "previousJOINcashBalanceJOINinstallmentsJOINcreditCardBalance = join_w_stats('SK_ID_PREV', previousJOINcashBalanceJOINinstallments, creditCardBalance, 'creditcard')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/envs/homedefaul/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/home/arbiterelegantiae/anaconda3/envs/homedefaul/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  del sys.path[0]\n"
    }
   ],
   "source": [
    "trainJOINprev = join_w_stats('SK_ID_CURR', train, previousJOINcashBalanceJOINinstallmentsJOINcreditCardBalance, 'prev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store joined previous loans dara since it will be merged to test as well\n",
    "utilities.to_csv(previousJOINcashBalanceJOINinstallmentsJOINcreditCardBalance, '../../datatmp/data/previousJoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "There are 267 columns with greater than 70% missing values.\n"
    }
   ],
   "source": [
    "# drop all the new computed features that we consider no-influent from trainJOINprev\n",
    "trainJOINprev = utilities.remove_missing_columns(trainJOINprev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureauBalance = pd.read_csv('../../data/bureau_balance.csv')\n",
    "bureau = pd.read_csv('../../data/bureau.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.66 gb.\nNew Memory Usage: 0.25 gb.\nOriginal Memory Usage: 0.23 gb.\nNew Memory Usage: 0.1 gb.\n"
    }
   ],
   "source": [
    "bureauBalance = utilities.convert_types(bureauBalance, print_info=True)\n",
    "bureau = utilities.convert_types(bureau, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/envs/homedefaul/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
    }
   ],
   "source": [
    "bureauJOINbureauBalance = join_w_stats('SK_ID_BUREAU', bureau, bureauBalance, 'bureauBalance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/envs/homedefaul/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
    }
   ],
   "source": [
    "trainJoined = join_w_stats('SK_ID_CURR', trainJOINprev, bureauJOINbureauBalance, 'bureau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "There are 39 columns with greater than 70% missing values.\n"
    }
   ],
   "source": [
    "trainJoined = utilities.remove_missing_columns(trainJoined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.to_csv(bureauJOINbureauBalance, '../../datatmp/data/bureauJoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the final train\n",
    "utilities.to_csv(trainJoined, '../../datatmp/data/trainjoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Original Memory Usage: 0.06 gb.\nNew Memory Usage: 0.03 gb.\n"
    }
   ],
   "source": [
    "## apply the same logic to the test ##\n",
    "test = pd.read_csv('../../datatmp/data/test.csv')\n",
    "test = utilities.convert_types(test, print_info=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# previously stored, read and infer the right types\n",
    "prevJoined = utilities.read_csv('../../datatmp/data/previousJoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/envs/homedefaul/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n/home/arbiterelegantiae/anaconda3/envs/homedefaul/lib/python3.7/site-packages/ipykernel_launcher.py:13: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  del sys.path[0]\n"
    }
   ],
   "source": [
    "testJOINprev = join_w_stats('SK_ID_CURR', test, prevJoined, 'prev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "There are 267 columns with greater than 70% missing values.\n"
    }
   ],
   "source": [
    "testJOINprev = utilities.remove_missing_columns(testJOINprev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bureauJOINbureauBalance = utilities.read_csv('../../datatmp/data/bureauJoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": "/home/arbiterelegantiae/anaconda3/envs/homedefaul/lib/python3.7/site-packages/ipykernel_launcher.py:19: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
    }
   ],
   "source": [
    "testJoined = join_w_stats('SK_ID_CURR', testJOINprev, bureauJOINbureauBalance, 'bureau')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "There are 0 columns with greater than 70% missing values.\n"
    }
   ],
   "source": [
    "testJoined = utilities.remove_missing_columns(testJoined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainJoined = utilities.read_csv('../../datatmp/data/trainjoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "777\n815\n"
    }
   ],
   "source": [
    "## it seems like there were more sparse features in train than in test that were removed, this is reasonable due to the larger ids in train\n",
    "print(len(trainJoined.columns))\n",
    "print(len(testJoined.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(307511, 777)"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to align as we did in homedefault_traintest\n",
    "\n",
    "target = trainJoined['TARGET']\n",
    "\n",
    "#Align the training and testing data, keep only columns present in both dataframes\n",
    "trainJoined, testJoined = trainJoined.align(testJoined, join = 'inner', axis = 1)\n",
    "\n",
    "#Add the target back in\n",
    "trainJoined['TARGET'] = target\n",
    "\n",
    "trainJoined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "(48744, 776)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testJoined.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.to_csv(trainJoined, '../../datatmp/data/trainjoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "utilities.to_csv(testJoined, '../../datatmp/data/testjoined.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue to the feat engineering phase"
   ]
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('homedefaul': conda)",
   "language": "python",
   "name": "python37664bithomedefaulcondac3ef0b2983f64cfd8cf91c045a31b21e"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}