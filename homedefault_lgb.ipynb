{
 "nbformat": 4,
 "nbformat_minor": 2,
 "metadata": {
  "language_info": {
   "name": "python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "version": "3.7.6-final"
  },
  "orig_nbformat": 2,
  "file_extension": ".py",
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "kernelspec": {
   "name": "python37664bithomedefaulcondac3ef0b2983f64cfd8cf91c045a31b21e",
   "display_name": "Python 3.7.6 64-bit ('homedefaul': conda)"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## LIGHT GBM ##\n",
    "# \"LightGBM is a gradient boosting framework that uses tree based learning algorithms\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import gc\n",
    "gc.enable()\n",
    "import lightgbm as lgb\n",
    "import utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = utilities.read_csv('../../datatmp/data/trainjoincollnoisy.csv')\n",
    "test = utilities.read_csv('../../datatmp/data/testjoincollnoisy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train['TARGET'].astype(int).copy()\n",
    "train.drop('TARGET',axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "testIds = test['SK_ID_CURR'].astype(int).copy()\n",
    "\n",
    "train.drop('SK_ID_CURR', axis=1, inplace=True)\n",
    "test.drop('SK_ID_CURR', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Due to the last version of lgb, we got an error related to special chars in column names.\n",
    "train.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in train.columns]\n",
    "test.columns = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in test.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = utilities.read_csv('../../datatmp/data/importances.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select all non-zero importance feats\n",
    "impFeats = []\n",
    "for feat in importances:\n",
    "    if importances.loc[0][feat] > 0:\n",
    "        impFeats.append(feat)\n",
    "impFeats = [\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in impFeats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final train test\n",
    "train = train[impFeats]\n",
    "test = test[impFeats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "def trainNpredLgb(train, test, y, nFolds):\n",
    "    \"\"\" Training and predictions of test with Light Gmb Kfold \"\"\"\n",
    "    import lightgbm as lgb\n",
    "\n",
    "    # Prepare test predictions series \n",
    "    testPreds = np.zeros(test.shape[0])\n",
    "    # Unbalanced dataset => better to fold (use stratified K-Folds?)\n",
    "    folds = KFold(n_splits=nFolds, shuffle=True, random_state=42)\n",
    "    for fold, (tIds, vIds) in enumerate(folds.split(train, y)):\n",
    "        trainX, trainY = train.iloc[tIds], y.iloc[tIds]\n",
    "        valX, valY = train.iloc[vIds], y.iloc[vIds]\n",
    "        # LightGBM parameters found by Bayesian optimization (from https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code)\n",
    "        clf = lgb.LGBMClassifier(\n",
    "            objective = 'binary',\n",
    "            boosting_type = 'gbdt',\n",
    "            nthread=4,\n",
    "            n_estimators=5000,\n",
    "            learning_rate=0.03,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "        clf.fit(\n",
    "            trainX,\n",
    "            trainY,\n",
    "            eval_set=[(trainX, trainY), (valX, valY)],\n",
    "            eval_metric='auc',\n",
    "            verbose=100,\n",
    "            early_stopping_rounds=200 \n",
    "        )\n",
    "        # Average best iteration preds for test \n",
    "        testPreds += clf.predict_proba(test, num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits \n",
    "        # Free space\n",
    "        del trainX, trainY, valX, valY\n",
    "        gc.collect()\n",
    "    del train, test, y\n",
    "    gc.collect()\n",
    "    return testPreds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": "Training until validation scores don't improve for 200 rounds\n[100]\ttraining's auc: 0.785123\ttraining's binary_logloss: 0.24009\tvalid_1's auc: 0.766725\tvalid_1's binary_logloss: 0.244508\n[200]\ttraining's auc: 0.807882\ttraining's binary_logloss: 0.23075\tvalid_1's auc: 0.778798\tvalid_1's binary_logloss: 0.23974\n[300]\ttraining's auc: 0.821821\ttraining's binary_logloss: 0.224989\tvalid_1's auc: 0.783112\tvalid_1's binary_logloss: 0.238003\n[400]\ttraining's auc: 0.833072\ttraining's binary_logloss: 0.220401\tvalid_1's auc: 0.785588\tvalid_1's binary_logloss: 0.237039\n[500]\ttraining's auc: 0.842505\ttraining's binary_logloss: 0.216501\tvalid_1's auc: 0.787064\tvalid_1's binary_logloss: 0.23647\n[600]\ttraining's auc: 0.850428\ttraining's binary_logloss: 0.213215\tvalid_1's auc: 0.787689\tvalid_1's binary_logloss: 0.236201\n[700]\ttraining's auc: 0.857905\ttraining's binary_logloss: 0.210023\tvalid_1's auc: 0.788533\tvalid_1's binary_logloss: 0.235904\n[800]\ttraining's auc: 0.864809\ttraining's binary_logloss: 0.207029\tvalid_1's auc: 0.788937\tvalid_1's binary_logloss: 0.235762\n[900]\ttraining's auc: 0.871595\ttraining's binary_logloss: 0.204025\tvalid_1's auc: 0.789117\tvalid_1's binary_logloss: 0.23565\n[1000]\ttraining's auc: 0.877852\ttraining's binary_logloss: 0.201169\tvalid_1's auc: 0.789018\tvalid_1's binary_logloss: 0.235679\nEarly stopping, best iteration is:\n[886]\ttraining's auc: 0.870652\ttraining's binary_logloss: 0.204431\tvalid_1's auc: 0.789231\tvalid_1's binary_logloss: 0.235623\nTraining until validation scores don't improve for 200 rounds\n[100]\ttraining's auc: 0.784409\ttraining's binary_logloss: 0.240399\tvalid_1's auc: 0.766973\tvalid_1's binary_logloss: 0.244359\n[200]\ttraining's auc: 0.807605\ttraining's binary_logloss: 0.230901\tvalid_1's auc: 0.778865\tvalid_1's binary_logloss: 0.239422\n[300]\ttraining's auc: 0.821649\ttraining's binary_logloss: 0.225054\tvalid_1's auc: 0.783193\tvalid_1's binary_logloss: 0.237633\n[400]\ttraining's auc: 0.832854\ttraining's binary_logloss: 0.220476\tvalid_1's auc: 0.785626\tvalid_1's binary_logloss: 0.236724\n[500]\ttraining's auc: 0.842199\ttraining's binary_logloss: 0.216661\tvalid_1's auc: 0.786884\tvalid_1's binary_logloss: 0.236202\n[600]\ttraining's auc: 0.850676\ttraining's binary_logloss: 0.213108\tvalid_1's auc: 0.787782\tvalid_1's binary_logloss: 0.235884\n[700]\ttraining's auc: 0.858651\ttraining's binary_logloss: 0.209732\tvalid_1's auc: 0.788461\tvalid_1's binary_logloss: 0.235636\n[800]\ttraining's auc: 0.865959\ttraining's binary_logloss: 0.206582\tvalid_1's auc: 0.788797\tvalid_1's binary_logloss: 0.235516\n[900]\ttraining's auc: 0.872671\ttraining's binary_logloss: 0.203595\tvalid_1's auc: 0.788918\tvalid_1's binary_logloss: 0.235489\n[1000]\ttraining's auc: 0.879065\ttraining's binary_logloss: 0.200714\tvalid_1's auc: 0.78898\tvalid_1's binary_logloss: 0.235468\n[1100]\ttraining's auc: 0.884922\ttraining's binary_logloss: 0.197885\tvalid_1's auc: 0.789167\tvalid_1's binary_logloss: 0.235405\n[1200]\ttraining's auc: 0.890557\ttraining's binary_logloss: 0.195163\tvalid_1's auc: 0.789099\tvalid_1's binary_logloss: 0.235427\n[1300]\ttraining's auc: 0.895687\ttraining's binary_logloss: 0.192605\tvalid_1's auc: 0.789096\tvalid_1's binary_logloss: 0.235442\nEarly stopping, best iteration is:\n[1139]\ttraining's auc: 0.887181\ttraining's binary_logloss: 0.196827\tvalid_1's auc: 0.789186\tvalid_1's binary_logloss: 0.235385\nTraining until validation scores don't improve for 200 rounds\n[100]\ttraining's auc: 0.785422\ttraining's binary_logloss: 0.238931\tvalid_1's auc: 0.764491\tvalid_1's binary_logloss: 0.249576\n[200]\ttraining's auc: 0.80811\ttraining's binary_logloss: 0.229536\tvalid_1's auc: 0.775915\tvalid_1's binary_logloss: 0.244935\n[300]\ttraining's auc: 0.821966\ttraining's binary_logloss: 0.223675\tvalid_1's auc: 0.780839\tvalid_1's binary_logloss: 0.243121\n[400]\ttraining's auc: 0.83305\ttraining's binary_logloss: 0.219177\tvalid_1's auc: 0.783092\tvalid_1's binary_logloss: 0.242356\n[500]\ttraining's auc: 0.842501\ttraining's binary_logloss: 0.215281\tvalid_1's auc: 0.784574\tvalid_1's binary_logloss: 0.241844\n[600]\ttraining's auc: 0.850615\ttraining's binary_logloss: 0.211926\tvalid_1's auc: 0.785447\tvalid_1's binary_logloss: 0.241555\n[700]\ttraining's auc: 0.858227\ttraining's binary_logloss: 0.208726\tvalid_1's auc: 0.786063\tvalid_1's binary_logloss: 0.24137\n[800]\ttraining's auc: 0.86533\ttraining's binary_logloss: 0.205638\tvalid_1's auc: 0.786379\tvalid_1's binary_logloss: 0.241265\n[900]\ttraining's auc: 0.871996\ttraining's binary_logloss: 0.202646\tvalid_1's auc: 0.786667\tvalid_1's binary_logloss: 0.241169\n[1000]\ttraining's auc: 0.878467\ttraining's binary_logloss: 0.199731\tvalid_1's auc: 0.786697\tvalid_1's binary_logloss: 0.241172\n[1100]\ttraining's auc: 0.884245\ttraining's binary_logloss: 0.196988\tvalid_1's auc: 0.78674\tvalid_1's binary_logloss: 0.241194\nEarly stopping, best iteration is:\n[911]\ttraining's auc: 0.8728\ttraining's binary_logloss: 0.202285\tvalid_1's auc: 0.786741\tvalid_1's binary_logloss: 0.241142\nTraining until validation scores don't improve for 200 rounds\n[100]\ttraining's auc: 0.784734\ttraining's binary_logloss: 0.240625\tvalid_1's auc: 0.764203\tvalid_1's binary_logloss: 0.243637\n[200]\ttraining's auc: 0.807355\ttraining's binary_logloss: 0.231265\tvalid_1's auc: 0.776417\tvalid_1's binary_logloss: 0.238741\n[300]\ttraining's auc: 0.820981\ttraining's binary_logloss: 0.225593\tvalid_1's auc: 0.781331\tvalid_1's binary_logloss: 0.236825\n[400]\ttraining's auc: 0.831843\ttraining's binary_logloss: 0.221108\tvalid_1's auc: 0.783541\tvalid_1's binary_logloss: 0.235942\n[500]\ttraining's auc: 0.841366\ttraining's binary_logloss: 0.217192\tvalid_1's auc: 0.78507\tvalid_1's binary_logloss: 0.235388\n[600]\ttraining's auc: 0.849375\ttraining's binary_logloss: 0.213833\tvalid_1's auc: 0.786139\tvalid_1's binary_logloss: 0.235006\n[700]\ttraining's auc: 0.856981\ttraining's binary_logloss: 0.210612\tvalid_1's auc: 0.786509\tvalid_1's binary_logloss: 0.234878\n[800]\ttraining's auc: 0.864058\ttraining's binary_logloss: 0.207604\tvalid_1's auc: 0.786822\tvalid_1's binary_logloss: 0.234781\n[900]\ttraining's auc: 0.870804\ttraining's binary_logloss: 0.204609\tvalid_1's auc: 0.787272\tvalid_1's binary_logloss: 0.234658\n[1000]\ttraining's auc: 0.877036\ttraining's binary_logloss: 0.201691\tvalid_1's auc: 0.787456\tvalid_1's binary_logloss: 0.2346\n[1100]\ttraining's auc: 0.883114\ttraining's binary_logloss: 0.198831\tvalid_1's auc: 0.787499\tvalid_1's binary_logloss: 0.234619\n[1200]\ttraining's auc: 0.888522\ttraining's binary_logloss: 0.196259\tvalid_1's auc: 0.787449\tvalid_1's binary_logloss: 0.234611\nEarly stopping, best iteration is:\n[1045]\ttraining's auc: 0.879576\ttraining's binary_logloss: 0.200508\tvalid_1's auc: 0.787564\tvalid_1's binary_logloss: 0.234594\nTraining until validation scores don't improve for 200 rounds\n[100]\ttraining's auc: 0.785116\ttraining's binary_logloss: 0.240131\tvalid_1's auc: 0.764536\tvalid_1's binary_logloss: 0.245349\n[200]\ttraining's auc: 0.807619\ttraining's binary_logloss: 0.230861\tvalid_1's auc: 0.776408\tvalid_1's binary_logloss: 0.240635\n[300]\ttraining's auc: 0.821651\ttraining's binary_logloss: 0.225013\tvalid_1's auc: 0.781325\tvalid_1's binary_logloss: 0.238735\n[400]\ttraining's auc: 0.832653\ttraining's binary_logloss: 0.22048\tvalid_1's auc: 0.783853\tvalid_1's binary_logloss: 0.237767\n[500]\ttraining's auc: 0.842286\ttraining's binary_logloss: 0.216525\tvalid_1's auc: 0.785307\tvalid_1's binary_logloss: 0.237184\n[600]\ttraining's auc: 0.850606\ttraining's binary_logloss: 0.213079\tvalid_1's auc: 0.786275\tvalid_1's binary_logloss: 0.236828\n[700]\ttraining's auc: 0.858084\ttraining's binary_logloss: 0.209866\tvalid_1's auc: 0.786825\tvalid_1's binary_logloss: 0.236642\n[800]\ttraining's auc: 0.865385\ttraining's binary_logloss: 0.206678\tvalid_1's auc: 0.787345\tvalid_1's binary_logloss: 0.236429\n[900]\ttraining's auc: 0.872009\ttraining's binary_logloss: 0.203747\tvalid_1's auc: 0.787647\tvalid_1's binary_logloss: 0.236322\n[1000]\ttraining's auc: 0.878268\ttraining's binary_logloss: 0.200908\tvalid_1's auc: 0.787829\tvalid_1's binary_logloss: 0.236275\n[1100]\ttraining's auc: 0.884127\ttraining's binary_logloss: 0.19818\tvalid_1's auc: 0.788075\tvalid_1's binary_logloss: 0.236228\n[1200]\ttraining's auc: 0.889556\ttraining's binary_logloss: 0.195558\tvalid_1's auc: 0.788122\tvalid_1's binary_logloss: 0.236203\n[1300]\ttraining's auc: 0.895012\ttraining's binary_logloss: 0.192927\tvalid_1's auc: 0.788214\tvalid_1's binary_logloss: 0.236181\n[1400]\ttraining's auc: 0.900025\ttraining's binary_logloss: 0.190362\tvalid_1's auc: 0.788171\tvalid_1's binary_logloss: 0.23619\n[1500]\ttraining's auc: 0.904469\ttraining's binary_logloss: 0.187935\tvalid_1's auc: 0.788168\tvalid_1's binary_logloss: 0.236227\nEarly stopping, best iteration is:\n[1375]\ttraining's auc: 0.898748\ttraining's binary_logloss: 0.191034\tvalid_1's auc: 0.788286\tvalid_1's binary_logloss: 0.236149\nCPU times: user 2h 36min 2s, sys: 15.9 s, total: 2h 36min 18s\nWall time: 13min 53s\n"
    }
   ],
   "source": [
    "# Train model on final train, test\n",
    "%time predictions = trainNpredLgb(train, test, target, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create submission csv \n",
    "subdf = pd.DataFrame({'SK_ID_CURR': testIds, 'TARGET': predictions})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store it\n",
    "subdf.to_csv('../../datatmp/data/submissionlgb.csv', index=False)"
   ]
  }
 ]
}